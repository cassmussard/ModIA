{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1vY_PgV2S6i"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Projet Fil Rouge 2023 : Reconnaissance de commandes audio\n",
    "\n",
    "\n",
    "**Noms :** GHAMNIA, MUSSARD\n",
    "\n",
    "**Prénoms :** Karima, Cassandra\n",
    "\n",
    "**Nom du binôme :** Cassandra&Karima\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ces 12 séances de TP vont vous permettre de tester l'algorithme de programmation dynamique vu en CTD puis de réaliser la mise en oeuvre d'un système de reconnaissance audio de mots isolés (constituant des\n",
    "commandes pour les drones).\n",
    "\n",
    "<img src=\"files/DroneJS.JPG\" width=\"600\" height=\"500\"  >\n",
    "\n",
    "\n",
    "\n",
    "Ces séances se décomposent en ces parties : \n",
    "- Partie I : Prétraitement des données \n",
    "- Partie II : Sélection de variables et pénalisation\n",
    "- Partie III : Classification par méthodes à noyau \n",
    "- Partie IV : Apprentissage par ensemble : Adaboost, gradient boosting\n",
    "- Partie V : Classification par réseaux de neurones\n",
    "- Partie VI : Votre étude\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Psuw-VFf2O0v"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import math\n",
    "import numpy.random as rnd\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3bO-vg72iSb"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "Sur l'espace moodle, vous trouverez un dossier d'enregistrements audio de mots de commandes pour un drone quadricoptère constitués de plusieurs locuteurs masculins (notés M01..M13) et locutrice féminines (F01..F05) pour quelques commandes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "label = [] \n",
    "genres = []\n",
    "min_duration = None\n",
    "words = ['avance','recule','tournegauche']\n",
    "list_genres = ['M', 'F']\n",
    "\n",
    "for file_name in glob.glob('FichierTest/*.wav'):\n",
    "    record = librosa.load(file_name)[0]\n",
    "    data.append(record)\n",
    "    # Computation of the minimal size of recordings\n",
    "    if min_duration is None or record.shape[0] < min_duration:\n",
    "        min_duration = record.shape[0] \n",
    "    \n",
    "    # Creation of the vector of label\n",
    "    for i, word in enumerate(words):\n",
    "      if re.search(word, file_name):\n",
    "        label.append(i)\n",
    "\n",
    "    # Creation of the vector of label\n",
    "    for i, genre in enumerate(list_genres):\n",
    "      if re.search(genre, file_name[12:]):# 12 is for ignoring \"FichierTest/\"\n",
    "        genres.append(genre)\n",
    "\n",
    "fs = librosa.load(file_name)[1] # Sampling frequency\n",
    "genres = np.array(genres)\n",
    "print(f'The smallest record contains {min_duration} samples, and the sample frequency is {fs} Hz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We trim the recordings to isolate the word and have identical durations\n",
    "The smallest record contains 18 522 samples. We are going to cut all recordings to be of this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trim(record):\n",
    "    half_duration = 18522//2\n",
    "\n",
    "    # First, we compute the barycenter of energy along time. We interpret it as the moment when the word appears\n",
    "    barycenter = int(np.floor(np.multiply(np.power(record,2),np.arange(0,record.shape[0],1)).sum()/np.power(record,2).sum()))\n",
    "\n",
    "    # Second, we adjust the barycenter to be in the index range\n",
    "    if barycenter-half_duration < 0:\n",
    "        barycenter += half_duration-barycenter\n",
    "    if barycenter+half_duration >= record.shape[0]:\n",
    "        barycenter -= barycenter+half_duration - record.shape[0]\n",
    "    \n",
    "    # Finally, we trim the recording around the barycenter \n",
    "    return record[barycenter-half_duration:barycenter+half_duration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.empty((len(data),min_duration))\n",
    "for i in range(len(data)):\n",
    "    X[i,:] = trim(data[i])\n",
    "\n",
    "y = np.array(label)\n",
    "print(f'Shape of inputs X is{X.shape} and size of targets class is {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral representation\n",
    "\n",
    "1. Apply a Fourrier transform on the signals in $X$ using the function fft of scipy. Explain why the resulting dimension is too large to apply logistic regression.\n",
    "\n",
    "2. Let $\\hat{X}$ be the fourier transform of $X$. Apply a PCA on $|\\hat{X}|$ and plot the total explained variance in function of the numer of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "# Transformée de Fourier sur le signal X\n",
    "X_fft = fft(X)\n",
    "Te = 1/22050\n",
    "freq = fftfreq(X.size, d=Te)\n",
    "freq = np.reshape(freq, (54, 18522))\n",
    "freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(freq[0,:], X.real[0,:], label=\"Partie réel\")\n",
    "plt.plot(freq[0,:], X.imag[0,:], label=\"Partie imaginaire\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explication :\n",
    "L'objectif d'un modèle de régression logistique est de trouver une relation entre les variables et les labels. Cepandent, quand on a un grand nombre de variable avec très peu d'observations, il est difficile de trouver ce lien. De plus, cela peut conduire à de l'overfitting. \n",
    "\n",
    "Dans notre cas, on a 18522 variables pour 54 observations, on risque donc  de rencontrer le problème expliquée ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Appliquer une PCA sur X\n",
    "n_components=11\n",
    "pca = PCA(n_components=n_components)\n",
    "X_fft_pca = pca.fit(abs(X_fft))\n",
    "var = pca.explained_variance_ratio_.sum()\n",
    "print(var)\n",
    "plt.plot(range(0,n_components), pca.explained_variance_ratio_)\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.title('Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut remarquer qu'en retenant 11 composantes principales, on atteint environ 80% de la variance expliquée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply a Short Term Fourier Transform on $X$. What are the dimension of stft $\\hat{X}[t,f]$?\n",
    "\n",
    "4. Make 2 subplots (3x3) of the stft (as images with function .imshow()) with three instances of each words, one for male and one for female "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nperseg = 253\n",
    "f, t, X_stft = signal.stft(X, fs=fs, window='hann', nperseg=nperseg, noverlap=None)\n",
    "X_stft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que la dimension de $Xfft$ est de : (54,127,147)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nperseg = 253\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, constrained_layout=True, figsize=(15,5))\n",
    "for i,word in enumerate(words):\n",
    "    for instance in range(3):\n",
    "        record = X[(y==i) & (genres == 'M')][instance]\n",
    "        f, t, Zxx = signal.stft(record, fs=fs, window='hann', nperseg=200, noverlap=None)\n",
    "        axs[i,instance].imshow(np.absolute(Zxx[:80]))\n",
    "        axs[i,instance].set_title(f'mfcc{instance,word}')\n",
    "\n",
    "        \n",
    "print(Zxx.shape)\n",
    "fig.suptitle('STFT de chaque mot pour les hommes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nperseg = 253\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, constrained_layout=True, figsize=(15,5))\n",
    "for i,word in enumerate(words):\n",
    "    for instance in range(3):\n",
    "        record = X[(y==i) & (genres == 'M')][instance]\n",
    "        f, t, Zxx = signal.stft(record, fs=fs, window='hann', nperseg=200, noverlap=None)\n",
    "        axs[i,instance].imshow(np.absolute(Zxx[:40]))\n",
    "        axs[i,instance].set_title(f'mfcc{instance,word}')\n",
    "\n",
    "        \n",
    "print(Zxx.shape)\n",
    "fig.suptitle('STFT de chaque mot pour les femmes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Remarque  :\n",
    "Pour les enregistrements des femmes, on remarque que les stft ont une forme différente pour chaque mot. Cependant, on remarque que certains stft peuvent être confondus avec d'autres (par exemple le 3ème stft de la classe avancé peut être confondu avec le 2ème et le 3ème  de la classe reculé)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will build sklearn transformers to extract features\n",
    "\n",
    "Create a class STFT in the same spirit as FFT. \n",
    "Add a first argument to choose between returning different statistics (mean, quantile, max...) along time such that each signal. Add a second argument that gives the maximum frequency index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFT(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, idx_frequence_max=None):\n",
    "        self.idx_frequence_max = idx_frequence_max    #def split_train_test():\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        # Perform arbitary transformation \n",
    "    def transform(self,X,y=None):       \n",
    "        return np.absolute(fft(X)[:self.idx_frequence_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fft = FFT()\n",
    "X_ = Fft.transform(X)\n",
    "X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFT(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, idx_frequence_max=None, statistique = np.mean):\n",
    "        self.idx_frequence_max = idx_frequence_max    #def split_train_test():\n",
    "        self.statistique = statistique\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        # Perform arbitary transformation \n",
    "    def transform(self,X,y=None):    \n",
    "        f, t, Zxx = signal.stft(X, fs=fs, window='hann', nperseg=200, noverlap=None)   \n",
    "        return self.statistique(np.absolute(Zxx[:,:self.idx_frequence_max,:]),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fft = STFT(idx_frequence_max=40)\n",
    "X_ = STFT(idx_frequence_max=40).fit(X, y).transform(X)\n",
    "X_.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie I : Sélection de modèles et pénalisation\n",
    "\n",
    "### 1. Multiclass regression\n",
    "\n",
    "\n",
    "Apply a **multiclass regression** model.\n",
    "\n",
    "We model the probabilities by the following form :\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(Y_i = j) = \\frac { \\exp^{-\\beta_j^{T} X_i } } {1 + \\sum_{\\ell = 1}^{K-1} \\exp^{-\\beta_\\ell^{T} X_i }}, \n",
    "$$\n",
    "For all $j$ in $\\{ 1,2, \\dots , K-1 \\}$.\n",
    "\n",
    "### Objective\n",
    "\n",
    "Try to apply a logistic regression with **Leave one out Cross validation** on :\n",
    "\n",
    "1. The first PCA components of FFT (try multiple \"n_compenents\")\n",
    "2. Different statistics and maximum frequency of the STFT\n",
    "3. The same as before with scaling\n",
    "\n",
    "In each situations try different regularization coefficient C.\n",
    "\n",
    "To simplify use the **pipeline** function of sklearn. You can also use the function **GridSearchCV** with cv = X.shape[0] to vary the parameters of preprocessing and logistic regression. You can acess to all results with \"cv_results_\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique une regression logistique sur $X$, auquel on applique une FFT puis, une PCA. On effectue une validation croisée ainsi qu'une recherche de grille $('Grid Search')$  pour retrouver les meilleurs hyperparamètres. On choisit de varier les hyperparamètres comme suit :\n",
    "\n",
    "$n\\_components$ :\n",
    "* 11 composantes (80% de la variance)\n",
    "* 18 composantes (90% de la variance)\n",
    "* 54 composantes (100% de la variance)\n",
    "\n",
    "$logistic\\_solver$ : On teste tous les algorithmes d'optimisation possibles.\n",
    "\n",
    "\n",
    "\n",
    "$logistic\\_C$ : On l'utilise pour éviter l'overfitting. On le varie d'une façon loglinéaire.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "Fft = FFT()\n",
    "\n",
    "logistic = LogisticRegression(multi_class='multinomial', max_iter = 6000)\n",
    "cv = LeaveOneOut()\n",
    "\n",
    "pipe = Pipeline(steps=[(\"fft\", Fft), (\"pca\", pca), (\"logistic\", logistic)])\n",
    "\n",
    "param_grid = {\n",
    "    \"pca__n_components\": [11, 18, 54],\n",
    "    \"logistic__C\": np.logspace(-4, 4, 4),\n",
    "    \"logistic__solver\": ['lbfgs', 'saga', 'sag', 'newton-cg'],\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, verbose =1, cv=cv, n_jobs=-1)\n",
    "res = search.fit(X, y)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat : On obtient  81% d'accuracy avec $c = 0.0001$, lbfgs et $n\\_components = 18$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On plot l'accuracy en fonction de c\n",
    "tab_c = []\n",
    "tab_mean= []\n",
    "for i in range(3):\n",
    "    tab_c.append(search.cv_results_[\"mean_test_score\"][i])\n",
    "    tab_mean.append([1.00000000e-04, 4.64158883e-02, 2.15443469e+01, 1.00000000e+04][i])\n",
    "plt.plot(tab_mean, tab_c)\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Plot des résultats de GridSearch\n",
    "mean_scores = res.cv_results_['mean_test_score']\n",
    "params = res.cv_results_['params']\n",
    "\n",
    "n_components = [p['pca__n_components'] for p in params]\n",
    "C_val = [p['logistic__C'] for p in params]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(n_components, C_val, mean_scores, c=mean_scores, cmap='viridis')\n",
    "\n",
    "ax.set_xlabel('Nombre de composantes d ACP')\n",
    "ax.set_ylabel('C')\n",
    "ax.set_zlabel('Score')\n",
    "ax.set_title('Résultats de Grid Search')\n",
    "\n",
    "ax.set_xticks(n_components)\n",
    "ax.set_yticks(C_val)\n",
    "\n",
    "cbar = fig.colorbar(scatter)\n",
    "cbar.set_label('Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Different statistics and maximum frequency of the STFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique une regression logistique sur $X$, auquel on applique une STFT. On effectue une validation croisée ainsi qu'une recherche de grille $('Grid Search')$  pour retrouver les meilleurs hyperparamètres. On choisit de varier les hyperparamètres comme suit :\n",
    "\n",
    "\n",
    "$logistic\\_solver$ : On garde lbfgs car, généralement les solvers n'influencent pas trop sur l'accuracy, mais plus sur le temps d'exécution.\n",
    "\n",
    "$logistic\\_C$ : On l'utilise pour éviter l'overfitting. On le varie d'une façon loglinéaire.\n",
    "\n",
    "\n",
    "$Stft\\_idx\\_frequence_max$ :\n",
    "$Stft\\_statistique$ : On teste des statistiques classiques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stft = STFT()\n",
    "# set the tolerance to a large value to make the example faster\n",
    "logistic = LogisticRegression(multi_class='multinomial', max_iter = 10000)\n",
    "cv = LeaveOneOut()\n",
    "pipe2 = Pipeline(steps=[(\"Stft\", Stft), (\"logistic\", logistic)])\n",
    "\n",
    "# Parameters of pipelines can be set using '__' separated parameter names:\n",
    "param_grid2 = {\n",
    "    \"logistic__C\": np.logspace(-4, 4, 4),\n",
    "    \"logistic__solver\": ['lbfgs'],\n",
    "    \"Stft__idx_frequence_max\": [10, 20, 80],\n",
    "    \"Stft__statistique\": [np.mean, np.median, np.max],\n",
    "}\n",
    "search2 = GridSearchCV(pipe2, param_grid2, verbose =1, cv=cv, n_jobs=-1)\n",
    "res2 = search2.fit(X, y)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search2.best_score_)\n",
    "print(search2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat : On obtient une accuracy de 81% avec la statistique max, c=10000, Stft__idx_frequence_max = 80 et l'algorithme lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On plot l'accuracy en fonction de c\n",
    "tab_c = []\n",
    "tab_mean= []\n",
    "for i in range(3):\n",
    "    tab_c.append(search2.cv_results_[\"mean_test_score\"][i])\n",
    "    tab_mean.append(search2.cv_results_[\"params\"][i][\"logistic__C\"])\n",
    "plt.plot(tab_mean, tab_c)\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot des résultats de GridSearch\n",
    "mean_scores = res2.cv_results_['mean_test_score']\n",
    "params = res2.cv_results_['params']\n",
    "\n",
    "freq = [p['Stft__idx_frequence_max'] for p in params]\n",
    "C_val = [p['logistic__C'] for p in params]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(freq, C_val, mean_scores, c=mean_scores, cmap='viridis')\n",
    "\n",
    "ax.set_xlabel('Nombre de frequences')\n",
    "ax.set_ylabel('C')\n",
    "ax.set_zlabel('Score')\n",
    "ax.set_title('Résultats de Grid Search')\n",
    "\n",
    "ax.set_xticks(n_components)\n",
    "ax.set_yticks(C_val)\n",
    "\n",
    "cbar = fig.colorbar(scatter)\n",
    "cbar.set_label('Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The same as before with scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On varie les hyperparamètres de la même façon que précedemment après avoir effectuer un scaling sur les $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scale = StandardScaler()\n",
    "Stft = STFT()\n",
    "# set the tolerance to a large value to make the example faster\n",
    "logistic = LogisticRegression(multi_class='multinomial', max_iter = 8000)\n",
    "cv = LeaveOneOut()\n",
    "pipe3 = Pipeline(steps=[(\"scale\", scale),(\"Stft\", Stft), (\"logistic\", logistic)])\n",
    "\n",
    "# Parameters of pipelines can be set using '__' separated parameter names:\n",
    "param_grid3 = {\n",
    "    \"logistic__C\": np.logspace(-4, 4, 4),\n",
    "    \"logistic__solver\": ['lbfgs'],\n",
    "    \"Stft__idx_frequence_max\": [10, 20,80],\n",
    "    \"Stft__statistique\": [np.mean, np.median, np.max],\n",
    "\n",
    "}\n",
    "search3 = GridSearchCV(pipe3, param_grid3, verbose =1, cv=cv, n_jobs=-1)\n",
    "res3 = search3.fit(X, y)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search3.best_score_)\n",
    "print(search3.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat : On obtient 88% d'accuracy avec une fréquence max de 80, une statistique = median, C=10000 et l'algorithme lbfgs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On plot l'accuracy en fonction de c\n",
    "tab_c = []\n",
    "tab_mean= []\n",
    "for i in range(3):\n",
    "    tab_c.append(search3.cv_results_[\"mean_test_score\"][i])\n",
    "    tab_mean.append(search3.cv_results_[\"params\"][i][\"logistic__C\"])\n",
    "plt.plot(tab_mean, tab_c)\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot des résultats de GridSearch\n",
    "mean_scores = res3.cv_results_['mean_test_score']\n",
    "params = res3.cv_results_['params']\n",
    "\n",
    "freq = [p['Stft__idx_frequence_max'] for p in params]\n",
    "C_val = [p['logistic__C'] for p in params]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(freq, C_val, mean_scores, c=mean_scores, cmap='viridis')\n",
    "\n",
    "ax.set_xlabel('Nombre de frequences')\n",
    "ax.set_ylabel('C')\n",
    "ax.set_zlabel('Score')\n",
    "ax.set_title('Résultats de Grid Search')\n",
    "\n",
    "ax.set_xticks(n_components)\n",
    "ax.set_yticks(C_val)\n",
    "\n",
    "cbar = fig.colorbar(scatter)\n",
    "cbar.set_label('Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluation des résultats \n",
    "\n",
    "Evaluer le résultat par matrice de confusion et pourcentage de bonne classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation des résultats par matrices de confusion \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def res_score(search) : \n",
    "    y_pred = search.best_estimator_.predict(X)\n",
    "    confusion_matrice = confusion_matrix(y, y_pred)\n",
    "    score = search.best_score_\n",
    "    return confusion_matrice, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des matrices de confusion et les scores\n",
    "conf_matrix1, score1 = res_score(search)\n",
    "conf_matrix2, score2 = res_score(search2)\n",
    "conf_matrix3, score3 = res_score(search3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour afficher les matrice de confusion\n",
    "\n",
    "def affiche_conf_mtx(conf, couleur):\n",
    "    sns.heatmap(conf, annot=True, fmt=\"d\", cmap=couleur)\n",
    "    plt.xlabel(\"Prédictions\")\n",
    "    plt.ylabel(\"Valeurs réelles\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "affiche_conf_mtx(conf_matrix1, 'Blues')\n",
    "affiche_conf_mtx(conf_matrix2, 'Greens')\n",
    "affiche_conf_mtx(conf_matrix3, 'Reds')\n",
    "\n",
    "print(\"Question 1 \\n\", \"\\n\", score1)\n",
    "print(\"Question 2 \\n\", \"\\n\", score2)\n",
    "print(\"Question 3 \\n\", \"\\n\", score3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Partie II : Classification par méthodes à noyau\n",
    "\n",
    "**Rappel** Les méthodes à noyau consistent à plonger les données dans un espace de dimension de Hilbert $\\mathcal{H}$ ou les donnés pourront être séparé linéairement. \n",
    "\n",
    "**Theorème de Représentation :** La solution du problème de séparation en dimension infinie est contenue dans un sous espace vectoriel de dimension finie de $\\mathcal{H}$ \n",
    "\n",
    "### 1. Réaliser une classification par SVM à noyau\n",
    "\n",
    "    1) Varier le noyau\n",
    "    2) Varier le paramètre de régularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique une classification par SVM à noyau sur $X$, auquel on applique une FFT, pui une PCA. On effectue une validation croisée ainsi qu'une recherche de grille $('Grid Search')$  pour retrouver les meilleurs hyperparamètres suivants : $svc\\_kernel$ et $svc\\_C$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "pca = PCA()\n",
    "Fft = FFT()\n",
    "\n",
    "svc = SVC()\n",
    "cv = LeaveOneOut()\n",
    "pipe = Pipeline(steps=[(\"fft\", Fft), (\"pca\", pca), (\"svc\", svc)])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"pca__n_components\": [15],\n",
    "    \"svc__C\": np.logspace(-4, 4, 4),\n",
    "    \"svc__kernel\": ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, verbose =1, cv=cv, n_jobs=-1, refit=True)\n",
    "res_svc = search.fit(X, y)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot des résultats de GridSearch\n",
    "mean_scores = res_svc.cv_results_['mean_test_score']\n",
    "params = res_svc.cv_results_['params']\n",
    "\n",
    "n_components = [p['pca__n_components'] for p in params]\n",
    "svc_c = [p['svc__C'] for p in params]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(n_components, svc_c, mean_scores, c=mean_scores, cmap='viridis')\n",
    "\n",
    "ax.set_xlabel('Nombre de composantes d ACP')\n",
    "ax.set_ylabel('C')\n",
    "ax.set_zlabel('Score')\n",
    "ax.set_title('Résultats de Grid Search')\n",
    "\n",
    "ax.set_xticks(n_components)\n",
    "ax.set_yticks(C_val)\n",
    "\n",
    "cbar = fig.colorbar(scatter)\n",
    "cbar.set_label('Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat : On obtient 77% d'accuracy avec $C=0.046$, $n\\_components = 15$, et un noyeau linéaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le modèle avec les meilleurs hyperparamètres\n",
    "best_model = res_svc.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundary\n",
    "\n",
    "Ici, on affiche les frontières entre les 3 classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "def plot_decision_boundary(X, y, model):\n",
    "    feature_1, feature_2 = np.meshgrid(\n",
    "            np.linspace(X[:, 0].min(), X[:, 0].max()),\n",
    "            np.linspace(X[:, 1].min(), X[:, 1].max())\n",
    "        )\n",
    "    grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n",
    "    tree = SVC().fit(X[:, :2], y)\n",
    "    y_pred = np.reshape(tree.predict(grid), feature_1.shape)\n",
    "    display = DecisionBoundaryDisplay(\n",
    "        xx0=feature_1, xx1=feature_2, response=y_pred)\n",
    "    display.plot()\n",
    "    plt.title(\"Frontières de décision\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X, y, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging et Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adaptative boosting : AdaBoost\n",
    "\n",
    "Here is the algorithm Adaboost\n",
    "\n",
    "1. Initialize the data weighting coefficients ${w_n}$ by setting $w_n^{(1)} = 1/N$ for $n = 1,...,N$.\n",
    "2. For $m = 1,...,M$:\n",
    "    \n",
    "    **(a)** Fit a classifier $y_m(x)$ to the training data by minimizing the weighted\n",
    "error function\n",
    "    \n",
    "    $J_m = \\sum_{n=1}^N{w_n^{(m)}I(y_m(x)\\neq t_n)}$\n",
    "\n",
    "    where $I(y_m(x)\\neq t_n)$ is the indicator function and equals $1$ when $y_m(x_n) \t= t_n$ and $0$ otherwise\n",
    "\n",
    "    **(b)** Evaluate the quantities\n",
    "\n",
    "    $\\epsilon_m = \\frac{\\sum_{n=1}^N{w_n^{(m)}I(y_m(x)\\neq t_n)}}{\\sum_{n=1}^N{w_n^{(m)}}}$\n",
    "\n",
    "    and then use these to evaluate\n",
    "\n",
    "    $\\alpha_m = \\textit{ln}\\left({\\frac{1-\\epsilon_m}{\\epsilon_m}}\\right)$\n",
    "\n",
    "    **(c)** Update the data weighting coefficients\n",
    "    \n",
    "    $w_n^{(m+1)} = w_n^{(m)} \\textit{exp}\\left({\\alpha_m I(y_m(x_n) \\neq t_n)}\\right)$\n",
    "\n",
    "3. Make predictions using the final model, which is given by\n",
    "\n",
    "    $Y_M(x) = \\text{sign}\\left(\\sum_{m=1}^M {\\alpha_m y_m(x)}\\right)$\n",
    "\n",
    "\n",
    "**Question 1 :** Code from scratch the Adaboost algorithm in the same configuration as Bagging in the previous section. Use the sklearn decision tree classifier and its argument *sample_weight*. Compare its performances with Bagging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps on modifie les labels. On transforme les labels de sorte à avoir des labels binaire (-1 et 1). On met 1 si y (correspond aux labels de base) = 1 et -1 sinon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.zeros((len(y)))\n",
    "for i in range(len(y)):\n",
    "    if y[i]==1:\n",
    "        y1[i]= 1\n",
    "    else :\n",
    "        y1[i]= 0\n",
    "y2=y1*2-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on réalise le bagging. À chaque itération m de la boucle, un nouvel arbre de décision de profondeur maximale 1 est créé et ajusté sur les données d'apprentissage X_train avec des poids w_i. L'arbre est ajouté à la liste J_m. Les prédictions du modèle sur les données d'apprentissage sont calculées et comparées aux étiquettes de classe réelles pour calculer l'erreur err_m. Ensuite, le coefficient alpha_m est calculé en utilisant l'erreur err_m. Les poids w_i sont mis à jour en multipliant les poids actuels par l'exponentielle de alpha_m pour les échantillons mal classés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging \n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y1,test_size=0.4, random_state=42)\n",
    "np.random.seed(seed=42)\n",
    "N = X_train.shape[0]\n",
    "trees = []\n",
    "y_test_pred = []\n",
    "\n",
    "for b in range(N):\n",
    "    sample = np.random.choice(np.arange(N), size = N, replace = True) # On tire un échantillon \n",
    "    X_train_b = X_train[sample] #On selectionne cet echantillon dans les données trains\n",
    "    y_train_b = y_train[sample] # On récupère le label associé\n",
    "    modele = DecisionTreeClassifier()\n",
    "    tree = modele.fit(X_train_b, y_train_b) #On entraine l'arbre sur cet echantillon\n",
    "    trees.append(tree)\n",
    "    y_test_pred.append(modele.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (y_test_pred == y_test).mean() #On compare la prédiction avec le label\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient une accuracy d'environ 55%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y2,test_size=0.4, random_state=42)\n",
    "np.random.seed(seed=42)\n",
    "M = 1000\n",
    "N = X_train.shape[0]\n",
    "J_m = []\n",
    "err= []\n",
    "alphas = []\n",
    "y_test_pred = []\n",
    "w_i = np.ones(N)/ N\n",
    "\n",
    "for m in range(0,M):\n",
    "    modele = DecisionTreeClassifier(max_depth = 1) #On crée un arbre \n",
    "    tree = modele.fit(X_train, y_train, sample_weight = w_i) #On entraine l'arbre sur X_train\n",
    "    J_m.append(tree)\n",
    "    y_train_pred = tree.predict(X_train) #On prédit \n",
    "    err_m= (sum(w_i*(np.not_equal(y_train, y_train_pred)).astype(int)))/sum(w_i) #On calcule l'erreur\n",
    "    err.append(err_m)\n",
    "    alpha_m = np.log((1 - err_m) / err_m) #On update alpha\n",
    "    alphas.append(alpha_m)\n",
    "    w_i= w_i * np.exp(alpha_m* (np.not_equal(y_train, y_train_pred)).astype(int)) #On update les poids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.zeros((len(J_m), len(y_test)))\n",
    "for i in range(len(J_m)):\n",
    "    y_test_pred[i] = alphas[i]*J_m[i].predict(X_test) #Pour chaque arbre on fait la prediction moyenne\n",
    "Ym = np.sign(np.sum(y_test_pred, axis=0))\n",
    "accuracy = np.mean(Ym == y_test) \n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réalise maintenant le bagging avec la librairie Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 :** \n",
    "With sklearn library, apply adaboost with decision tree (*max_depth=2*) on the 3-class classification problem. Find good parameters with the leave one out cross validation. Do the same thing with Gradient bossting.\n",
    "If you have the time, you can test with XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "AdaBoostClassifier = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n",
    "AdaBoostClassifier.fit(X_train, y_train)\n",
    "y_pred = AdaBoostClassifier.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu'on obtient à peu près le même résultat que précédemment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous réalisons maintenant un random forest qui repose sur le bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_,y2,test_size=0.4, random_state=42)\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest.fit(X_train, y_train)\n",
    "y_pred_rf = randomForest.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retrouve à peu près la même accuracy que celle obtenue pour le bagging (72% d'accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Boosting\n",
    "\n",
    "Le Gradient Boosting permet l'optimisation de fonctions de perte différentiables arbitraires. Il permet d'optimiser la fonction de perte de l'apprenant précédent en ajoutant un nouveau modèle adaptatif qui combine des apprenants faibles.\n",
    "\n",
    "Etudier sur la fonction de perte et le taux d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.core.types.containers import ListTypeIterableType\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gradient = GradientBoostingClassifier()\n",
    "cv = LeaveOneOut()\n",
    "pipe = Pipeline(steps=[(\"gradient\", gradient)])\n",
    "\n",
    "param_grid = {\n",
    "    \"gradient__n_estimators\": [20],\n",
    "    \"gradient__learning_rate\": [1e-2],\n",
    "    \"gradient__loss\": [\"log_loss\"],\n",
    "    \"gradient__criterion\": [\"friedman_mse\"]\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, verbose =1, cv=cv, n_jobs=-1)\n",
    "res = search.fit(X, y)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient une accuracy de 45%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "XGBClassifier = XGBClassifier(DecisionTreeClassifier(max_depth=2))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_,y,test_size=0.4, random_state=42) # On sépare les données en train/test (60%/40%)\n",
    "y_train = y_train\n",
    "XGBClassifier.fit(X_train, y_train)\n",
    "y_pred_xgb = XGBClassifier.predict(X_test)\n",
    "accuracy_XGB= np.mean(y_pred_xgb == y_test)\n",
    "accuracy_XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient une accuracy d'environ : 68%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie IV : Neural Network with pytorch\n",
    "\n",
    "Below we create torch tensor with the shape $(N,B,F)$, where\n",
    "    \n",
    "$N$ is the number of recordings in the set (train/test)\n",
    "\n",
    "$B$ the size of batch, we choose $B=1$ because the dataset is really small\n",
    "\n",
    "$F$ is the number of features\n",
    "\n",
    "The tensors are converted to float type\n",
    "\n",
    "The train set and test set constitute 50% of the initial dataset\n",
    "\n",
    "**Transform X with your preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "### transform X with your preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_,y,test_size=0.5, random_state=42)\n",
    "X_train = torch.tensor(X_train).reshape((X_train.shape[0],1,-1)).float()\n",
    "X_test = torch.tensor(X_test).reshape((X_test.shape[0],1,-1)).float()\n",
    "y_train = torch.nn.functional.one_hot(torch.tensor(y_train), num_classes=- 1).reshape((X_train.shape[0],1,-1)).float()\n",
    "y_test = torch.nn.functional.one_hot(torch.tensor(y_test), num_classes=- 1).reshape((X_test.shape[0],1,-1)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** : Create a model class (descending from torch.nn.Module). In a first time choose the appropriate architecture and the appropriate loss (the loss appear later) to reproduce logistic regression\n",
    "\n",
    "Usually a FNN is a succession of blocks (linear -> ReLU). Finally the networks transforms the initial vector into the output $\\hat{y} \\in \\mathbb{R}^3, \\hat{y}=(\\mathbb{P}(y=0|x),\\mathbb{P}(y=1|x),\\mathbb{P}(y=2|x))$ where $y$ is the word we want to predict and $x \\in \\mathbb{R}^{18522}$ is the accoustic signal\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on prend une couche Linéaire avec 18522 neurones sur la première couche (car cela correspondant à la taille de X en entrée) et en sortie on ne veut que 3 classes. On applique ensuite une Softmax pour obtenir une matrice avec des probabilités pour chaque classe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "        torch.nn.Linear(40, 3),\n",
    "        torch.nn.Softmax(dim = 1)\n",
    "        )\n",
    "        \n",
    "            ### Define here the succession of torch.nn modules that will constitutes your network\n",
    "            ### building blocks are torch.nn.ReLU, torch.nn.Linear\n",
    "\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        ### the forward method will be called each time you will write model(x). \n",
    "        ### It's equivalent to the function predict of sklearn\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réalise ensuite la boucle d'entrainement suivie de la boucle de test.\n",
    "\n",
    "Pour la boucle d'entrainement, on réalise 2000 epochs (on montre 2000 fois le dataset en entier au réseau), on utilise un pas de 0.001 (ce pas à été changé pour obtenir une bonne accuracy), la cross-entropy comme fonction d'activation (cette loss est très utilisée pour la classification multi-classe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = NNClassification()\n",
    "\n",
    "num_epochs = 2000\n",
    "result_test_loss = []\n",
    "result_train_loss=[]\n",
    "result_train_accuracy=[]\n",
    "result_test_accuracy=[]\n",
    "\n",
    "lr = 8e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "loss = torch.nn.CrossEntropyLoss()### What loss do you think is well suited for the classification problem (same as logistic regression)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    predictions_train= []\n",
    "    predictions_test = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train[i]) #Les images train sont données en entrée du réseau\n",
    "        loss_ =  loss(outputs, y_train[i]) # On calcule la loss\n",
    "        train_losses.append(loss_)\n",
    "        res = torch.argmax(outputs) == torch.argmax(y_train[i])#On prend l'argmax de la prédiction pour obtenir l'indice qui correspond à la classe prédite\n",
    "        predictions_train.append(res)\n",
    "        loss_.backward() # backpropagation\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    test_losses = [] ##Phase de test \n",
    "    for i in range(X_test.shape[0]):\n",
    "        outputs = model(X_test[i])#On passe les données dans le réseau\n",
    "        res_test = torch.argmax(outputs) == torch.argmax(y_test[i])  #On prend l'argmax de la prédiction pour obtenir l'indice qui correspond à la classe prédite\n",
    "        predictions_test.append(res_test)\n",
    "        loss_test = loss(outputs, y_test[i]) #On calcule la loss de test sans faire la backpropagation\n",
    "        test_losses.append(loss_test)\n",
    "    \n",
    "    result_train_loss.append(torch.stack(train_losses).mean().item())\n",
    "    result_test_loss.append( torch.stack(test_losses).mean().item())\n",
    "    result_train_accuracy.append(100*torch.stack(predictions_train).sum().item()/len(y_train))\n",
    "    result_test_accuracy.append( 100*torch.stack(predictions_test).sum().item()/len(y_test))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Plot the train and test loss. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche ensuite les loss de train et de test en fonction du nombre d'epochs. On voit que les loss diminuent bien au fur et à mesure que les epochs augmentent. Cependant, on peut commencer à observer un phénomène d'overfitting car, on remarque une légère augmentation de la loss de test et une diminution de la loss de train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.plot(result_train_loss, label='train')\n",
    "plt.plot(result_test_loss, label='test')\n",
    "plt.legend()\n",
    "plt.title(\"Loss de train et de test en fonction du nombre d'epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 :** Compute the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on affiche l'accuracy du train et du test en fonction des epochs. On peut confirmer les remarques observées dans la question précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.plot(result_train_accuracy, label='train')\n",
    "plt.plot(result_test_accuracy, label='test')\n",
    "plt.title(\"Accuracy de train et de test en fonction du nombre d'epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** If you encounter overfitting try to regularize your model with Dropout and/or L2/L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "### transform X with your preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_,y,test_size=0.5, random_state=42)\n",
    "X_train = torch.tensor(X_train).reshape((X_train.shape[0],1,-1)).float()\n",
    "X_test = torch.tensor(X_test).reshape((X_test.shape[0],1,-1)).float()\n",
    "y_train = torch.nn.functional.one_hot(torch.tensor(y_train), num_classes=- 1).reshape((X_train.shape[0],1,-1)).float()\n",
    "y_test = torch.nn.functional.one_hot(torch.tensor(y_test), num_classes=- 1).reshape((X_test.shape[0],1,-1)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On change l'architecture du réseau de neurones pour ne pas avoir d'overfitting. On rajoute la fonction d'activation ReLU et on ajoute aussi des couches. Pour éviter l'overfitting, on met une couche de Dropout qui permet de faire de la régularisation. Ici, 10% des neurones vont être désactivés à chaque epoch pour éviter que cela soit toujours les mêmes neurones qui reçoivent l'information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "        torch.nn.Linear(40, 100),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.Linear(100, 60),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.Linear(60, 20),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(0.2),\n",
    "        torch.nn.Linear(20, 3),\n",
    "        torch.nn.Softmax(dim = 1),\n",
    "        torch.nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réalise ensuite la boucle d'entrainement suivie de la boucle de test.\n",
    "\n",
    "Pour la boucle d'entrainement, on réalise 2000 epochs, on utilise un pas de 0.001 (ce pas à été changé pour obtenir une bonne accuracy), la cross-entropy comme fonction d'activation (cette loss est très utilisée pour la classification multi-classe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNClassification()\n",
    "\n",
    "num_epochs = 1000\n",
    "result_test_loss = []\n",
    "result_train_loss=[]\n",
    "result_train_accuracy=[]\n",
    "result_test_accuracy=[]\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "loss = torch.nn.CrossEntropyLoss()### What loss do you think is well suited for the classification problem (same as logistic regression)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    predictions_train= []\n",
    "    predictions_test = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train[i]) #Les images train sont données en entrée du réseau\n",
    "        loss_ =  loss(outputs, y_train[i]) # On calcule la loss\n",
    "        train_losses.append(loss_)\n",
    "        res = torch.argmax(outputs) == torch.argmax(y_train[i]) #On prend l'argmax de la prédiction pour obtenir l'indice qui correspond à la classe prédite\n",
    "        predictions_train.append(res)\n",
    "        loss_.backward() # backpropagation\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        outputs = model(X_test[i])\n",
    "        res_test = torch.argmax(outputs) == torch.argmax(y_test[i]) #On prend l'argmax de la prédiction pour obtenir l'indice qui correspond à la classe prédite\n",
    "        predictions_test.append(res_test)\n",
    "        loss_test = loss(outputs, y_test[i]) #On calcule la loss pour le test sans faire la backpropagation\n",
    "        test_losses.append(loss_test)\n",
    "    \n",
    "    result_train_loss.append(torch.stack(train_losses).mean().item())\n",
    "    result_test_loss.append( torch.stack(test_losses).mean().item())\n",
    "    result_train_accuracy.append(100*torch.stack(predictions_train).sum().item()/len(y_train))\n",
    "    result_test_accuracy.append( 100*torch.stack(predictions_test).sum().item()/len(y_test))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche les loss de train et de test en fonction du nombre d'epochs.\n",
    "On voit que la loss de train fait beaucoup d'oscillation (tout en diminuant progressivement en fonction du nombre d'epochs). On a là aussi éviter l'overfitting car on voit bien que la loss de test d'augmente pas quand la loss de train diminue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.plot(result_train_loss, label='train')\n",
    "plt.plot(result_test_loss, label='test')\n",
    "plt.title(\"Loss de train et de test en fonction du nombre d'epochs\")\n",
    "plt.xlabel(\"nombre d'epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche l'accuracy de train et de test en fonction du nombre d'epochs. On voit que l'accuracy de test atteint 90% comme précédemment (le modèle semble donc performant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.plot(result_train_accuracy, label='train_accuracy')\n",
    "plt.plot(result_test_accuracy, label = 'test_accuracy')\n",
    "plt.title(\"Accuracy de train et de test en fonction du nombre d'epochs\")\n",
    "plt.xlabel(\"nombre d'epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5(Bonus)** : Create a CNN that takes in input the accoustic signal without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "### transform X with your preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5, random_state=42)\n",
    "X_train = torch.tensor(X_train).reshape((X_train.shape[0],1,-1)).float()\n",
    "X_test = torch.tensor(X_test).reshape((X_test.shape[0],1,-1)).float()\n",
    "y_train = torch.nn.functional.one_hot(torch.tensor(y_train), num_classes=- 1).reshape((X_train.shape[0],1,-1)).float()\n",
    "y_test = torch.nn.functional.one_hot(torch.tensor(y_test), num_classes=- 1).reshape((X_test.shape[0],1,-1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les CNN, on n'applique pas de transformations sur nos données, car on va extraire les features du signal entier directement dans les couches convolutionnelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5, random_state=40)\n",
    "X_train = torch.tensor(X_train).reshape((X_train.shape[0],1,-1)).float()\n",
    "X_test = torch.tensor(X_test).reshape((X_test.shape[0],1,-1)).float()\n",
    "y_train = torch.nn.functional.one_hot(torch.tensor(y_train), num_classes=- 1).reshape((X_train.shape[0],1,-1)).float()\n",
    "y_test = torch.nn.functional.one_hot(torch.tensor(y_test), num_classes=- 1).reshape((X_test.shape[0],1,-1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = torch.nn.Sequential(\n",
    "           \n",
    "            torch.nn.Conv1d(1, 20, kernel_size=4, stride=3), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.7),\n",
    "            torch.nn.MaxPool1d(kernel_size=3, stride=2), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.7),\n",
    "            torch.nn.Conv1d(20, 1, kernel_size=4, stride=3), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.7),\n",
    "            torch.nn.Linear(1028, 3) # 1 x 3\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "\n",
    "        return torch.nn.functional.softmax(self.network(xb), 1)\n",
    "\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNClassification()\n",
    "num_epochs = 300\n",
    "\n",
    "result_test_loss = []\n",
    "result_train_loss= []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "loss = torch.nn.CrossEntropyLoss() ### What loss do you think is well suited for the classification problem (same as logistic regression)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    predictions_train = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "       ### code the training step (compute loss -> optimization step -> save the loss )\n",
    "        optimizer.zero_grad()\n",
    "        sortie = model(X_train[i])\n",
    "        loss_train = loss(sortie, y_train[i])\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss_train)\n",
    "        predictions_train.append(torch.argmax(sortie) == torch.argmax(y_train[i]))\n",
    "        \n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    predictions_test = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        ### code the eval step  (compute loss -> save the loss )\n",
    "        sortie = model(X_test[i])\n",
    "        predictions_test.append(torch.argmax(sortie) == torch.argmax(y_test[i]))\n",
    "        loss_test = loss(sortie, y_test[i])\n",
    "        test_losses.append(loss_test)\n",
    "        \n",
    "    result_train_loss.append(torch.stack(train_losses).mean().item())\n",
    "    result_test_loss.append( torch.stack(test_losses).mean().item())\n",
    "    test_accuracy.append(100 * torch.stack(predictions_test).sum().item() / len(y_test))\n",
    "    train_accuracy.append(100 * torch.stack(predictions_train).sum().item() / len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.plot(train_accuracy, label='train_accuracy')\n",
    "plt.plot(test_accuracy, label = 'test_accuracy')\n",
    "plt.title(\"Accuracy de train et de test en fonction du nombre d'epochs\")\n",
    "plt.xlabel(\"nombre d'epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ProjetFilRouge_ModIA_2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "df1bd51a7f2d13fd1aa468cc1b721fbbebc3502e1a2382494021d8cba0e61282"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
